{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LptMkyMxI1uH"
      },
      "outputs": [],
      "source": [
        "filename = \"treas_parking_meters_loc_datasd.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "# Replace 'your_csv_file.csv' with the path to your actual CSV file\n",
        "df_ori = pd.read_csv(filename)\n",
        "# print(df_ori.info)\n",
        "df_loc = df_ori.iloc[:,[3,7,8]]\n",
        "print(df_loc)\n",
        "\n",
        "def is_convertible_to_float_lat(s):\n",
        "    # return False\n",
        "    try:\n",
        "        float(s)\n",
        "        return True and ( float(s) > 27 and float(s) < 37 )\n",
        "    # except ValueError:\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_convertible_to_float_lng(s):\n",
        "    # return False\n",
        "    try:\n",
        "        float(s)\n",
        "        return True and ( float(s) > -126 and float(s) < -109 )\n",
        "    # except ValueError:\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "def criteria1_fn(s):\n",
        "    for items in months:\n",
        "        if( s.find(items) != -1  ):\n",
        "            return False\n",
        "        if(len(s.strip()) > 10):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "criteria1 = df_loc['pole'].apply(criteria1_fn)\n",
        "criteria2 = df_loc['lat'].apply(is_convertible_to_float_lat)\n",
        "criteria3 = df_loc['lng'].apply(is_convertible_to_float_lng)\n",
        "\n",
        "df_loc = df_loc[criteria1]\n",
        "df_loc = df_loc[criteria2]\n",
        "df_loc = df_loc[criteria3]\n",
        "\n",
        "# print(df_loc)\n",
        "\n",
        "min_lat = min(df_loc['lat'])\n",
        "min_lng = min(df_loc['lng'])\n",
        "\n",
        "# print(min_lat, min_lng)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def scatter_plot_columns(df, x_column_name, y_column_name):\n",
        "    # Extracting data from the specified columns\n",
        "    x_data = df[x_column_name]\n",
        "    y_data = df[y_column_name]\n",
        "\n",
        "    # Plotting the data as a scatter plot\n",
        "    plt.scatter(x_data, y_data, s=2)\n",
        "    plt.xlabel(x_column_name)\n",
        "    plt.ylabel(y_column_name)\n",
        "    plt.title(f'Scatter Plot of {x_column_name} vs {y_column_name}')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# df_loc['lng'] = -abs(df_loc['lng'])\n",
        "# df_loc['lat'] = abs(df_loc['lat'])\n",
        "\n",
        "df_loc = df_loc.drop_duplicates()\n",
        "\n",
        "# scatter_plot_columns(df_loc,'lng','lat')\n",
        "# print(df_loc)\n",
        "\n",
        "import pickle\n",
        "\n",
        "filename = \"treas_parking_payments_2019_datasd.csv\"\n",
        "df_payments = pd.read_csv(filename)\n",
        "\n",
        "# with open('payments.pickle', 'wb') as f:\n",
        "#     pickle.dump(df_payments, f)\n",
        "\n",
        "# df_payments = []\n",
        "\n",
        "# with open('payments.pickle', 'rb') as f:\n",
        "#     df_payments = pickle.load(f)\n",
        "\n",
        "print(df_payments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkkDIaasJES5"
      },
      "outputs": [],
      "source": [
        "df_pay = df_payments.iloc[:,[0,2,3]]\n",
        "df_pay\n",
        "df_pay = df_pay[df_pay['pole_id'].apply(criteria1_fn)]\n",
        "df_pay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_pay['date_trans_start'].str.split(' ')\n",
        "df_pay[['day_trans_start', 'time_trans_start']] = df_pay['date_trans_start'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# Dropping the original 'Full Name' column if needed\n",
        "df_pay.drop('date_trans_start', axis=1, inplace=True)\n",
        "df_pay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pay[['day_meter_expire', 'time_meter_expire']] = df_pay['date_meter_expire'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# Dropping the original 'Full Name' column if needed\n",
        "df_pay.drop('date_meter_expire', axis=1, inplace=True)\n",
        "df_pay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "def convert_to_24h(time_str):\n",
        "  # Parse the time string into a datetime object\n",
        "  time_obj = datetime.strptime(time_str, '%I:%M:%S %p')\n",
        "  # Convert to 24-hour format and format as string\n",
        "  return time_obj.strftime('%H:%M:%S')\n",
        "\n",
        "# Apply the conversion function to the 'time' column using apply\n",
        "df_pay['time_meter_expire'] = df_pay['time_meter_expire'].apply(convert_to_24h)\n",
        "df_pay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('df_pay_pole_d_start_end_separate', 'wb') as f:\n",
        "#     pickle.dump(df_pay, f)\n",
        "import pickle\n",
        "import pandas as pd\n",
        "# df_pay = []\n",
        "# with open('df_pay_pole_d_start_end_separate', 'rb') as f:\n",
        "#     df_pay = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pay_short = df_pay[:]\n",
        "def getHR(s):\n",
        "    return s.split(':')[0]\n",
        "df_pay_short['time_trans_start'] = df_pay_short['time_trans_start'].apply(getHR)\n",
        "df_pay_short['time_meter_expire'] = df_pay_short['time_meter_expire'].apply(getHR)\n",
        "\n",
        "df_pay_short.drop('day_trans_start', axis=1, inplace=True)\n",
        "df_pay_short.drop('day_meter_expire', axis=1, inplace=True)\n",
        "\n",
        "df_pay_short\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merged_df = df_pay_short.groupby('pole_id')['time_trans_start'].apply(lambda x: ','.join(x)).apply(lambda x: x.split(',')).reset_index()\n",
        "# # merged_df\n",
        "# merged_df_1 = df_pay_short.groupby('pole_id')['time_trans_start'].apply(lambda x: ','.join(x)).apply(lambda x: x.split(',')).reset_index()\n",
        "# merged_df_1\n",
        "merged_df = df_pay_short.groupby('pole_id').agg({'time_trans_start': ','.join, 'time_meter_expire': ','.join}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['time_trans_start'] = merged_df['time_trans_start'].apply(lambda x : x.split(','))\n",
        "merged_df['time_meter_expire'] = merged_df['time_meter_expire'].apply(lambda x : x.split(','))\n",
        "merged_df_new = merged_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "merged_df_new['time_trans_start'] = merged_df_new['time_trans_start'].apply(lambda x : Counter(x))\n",
        "merged_df_new['time_meter_expire'] = merged_df_new['time_meter_expire'].apply(lambda x : Counter(x))\n",
        "merged_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_car_status = merged_df_new.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add missing keys with zero values to each dictionary\n",
        "specified_keys = list(range(24))\n",
        "df_car_status['time_trans_start'] = [dict(diff, **{str(key): diff.get(str(key), 0) for key in specified_keys}) for diff in df_car_status['time_trans_start']]\n",
        "df_car_status['time_meter_expire'] = [dict(diff, **{str(key): diff.get(str(key), 0) for key in specified_keys}) for diff in df_car_status['time_meter_expire']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_car_status['data_diff'] = [{key: df_car_status['time_trans_start'][i][key] - df_car_status['time_meter_expire'][i].get(key, 0) for key in df_car_status['time_trans_start'][i]} for i in range(len(df_car_status))]\n",
        "df_car_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_dict(diff_dict):\n",
        "    # Convert keys to integers\n",
        "    diff_dict_int = {int(k): v for k, v in diff_dict.items()}\n",
        "    \n",
        "    # Find the minimum key\n",
        "    min_key = min(diff_dict_int.keys())\n",
        "    # max_key = max(diff_dict_int.keys())\n",
        "    \n",
        "    # Initialize the value to add\n",
        "    add_value = diff_dict_int[min_key]\n",
        "   \n",
        "    \n",
        "    # Iterate through the sorted keys (excluding the minimum key)\n",
        "    # for key in sorted(diff_dict_int.keys())[1:]:\n",
        "\n",
        "    for key in sorted(diff_dict_int.keys())[:]:\n",
        "        # Add the value of the current key to the value of the previous key\n",
        "        diff_dict_int[key] += add_value\n",
        "        # Update the value to add for the next key\n",
        "        add_value = diff_dict_int[key]\n",
        "    \n",
        "    # Convert keys back to strings and return the modified dictionary\n",
        "    return {str(k): v for k, v in diff_dict_int.items()}\n",
        "\n",
        "# Apply the transformation function to each dictionary in the data_diff column\n",
        "df_car_status['data_diff'] = df_car_status['data_diff'].apply(transform_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_car_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_values = df_car_status['data_diff'].apply(lambda x: min(x.values()))\n",
        "\n",
        "# Add the absolute value of the minimum value to each value in the dictionary\n",
        "df_car_status['data_diff'] = [{key: int(value) + abs(int(min_value)) for key, value in diff.items()} for diff, min_value in zip(df_car_status['data_diff'], min_values)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the sum of all values in each dictionary and divide each value by the sum\n",
        "\n",
        "def normalize_dictionary_values(d):\n",
        "    total = sum(d.values())\n",
        "    return {key: round(value / total , 4) for key, value in d.items()}\n",
        "df_car_status['data_diff'] = df_car_status['data_diff'].apply(normalize_dictionary_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_car_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_car_status['data_diff'][405]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Extract keys and values from the data_diff column\n",
        "diff_dict = df_car_status['data_diff'][835]\n",
        "\n",
        "# Sort dictionary items by integer keys\n",
        "sorted_diff_dict = {k: diff_dict[k] for k in sorted(diff_dict, key=int)}\n",
        "\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(4,3))\n",
        "# plt.bar(sorted_diff_dict.keys(), sorted_diff_dict.values())\n",
        "# plt.xlabel('Keys')\n",
        "# plt.ylabel('Values')\n",
        "# plt.title('Data Difference')\n",
        "# plt.show()\n",
        "\n",
        "# Convert keys and values to arrays for plotting\n",
        "x = np.array(list(sorted_diff_dict.keys()), dtype=int)\n",
        "y = np.array(list(sorted_diff_dict.values()))\n",
        "\n",
        "# Fit a polynomial curve\n",
        "coefficients = np.polyfit(x, y, 15)  # Choose the degree of the polynomial (e.g., 3 for cubic)\n",
        "curve_x = np.linspace(min(x), max(x), 100)\n",
        "curve_y = np.polyval(coefficients, curve_x)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(4,3))\n",
        "# plt.plot(curve_x, curve_y, label='Fitting Curve')\n",
        "plt.plot(curve_x, curve_y)\n",
        "# plt.scatter(x, y, color='red', label='Data Points')\n",
        "plt.xlabel('Hours (24-HR)')\n",
        "plt.ylabel('Free Slots')\n",
        "plt.title('Availability')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_loc.reset_index(drop=True, inplace=True)\n",
        "df_loc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_loc.rename(columns={'pole': 'pole_id'}, inplace=True)\n",
        "all_data_df = pd.merge(df_car_status, df_loc, on='pole_id', how='inner')\n",
        "all_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def print_df_to_csv(df, filename):\n",
        "    \"\"\"\n",
        "    Print DataFrame to a CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): The DataFrame to print.\n",
        "        filename (str): The name of the CSV file to create.\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"DataFrame has been printed to '{filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame has been printed to 'output.csv'\n"
          ]
        }
      ],
      "source": [
        "def add_prefix(column, prefix):\n",
        "    return column.apply(lambda x: prefix + str(x))\n",
        "all_data_df['pole_id'] = add_prefix(all_data_df['pole_id'],' ')\n",
        "print_df_to_csv(all_data_df, 'output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new = all_data_df.copy()\n",
        "all_data_df_new.drop(columns=['time_trans_start', 'time_meter_expire'], inplace=True)\n",
        "all_data_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame has been printed to 'car_time_loc.csv'\n"
          ]
        }
      ],
      "source": [
        "print_df_to_csv(all_data_df_new, 'car_time_loc.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scatter_plot_columns(df_loc,'lng','lat')\n",
        "print(df_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def scatter_plot_columns_range(df, x_column_name, y_column_name, x_min=None, x_max=None, y_min=None, y_max=None, dpi=100, save_path=None, dot_size=2):\n",
        "    # Extracting data from the specified columns\n",
        "    x_data = df[x_column_name]\n",
        "    y_data = df[y_column_name]\n",
        "\n",
        "    # Plotting the data as a scatter plot\n",
        "    plt.scatter(x_data, y_data, s=dot_size)\n",
        "    plt.xlabel(x_column_name)\n",
        "    plt.ylabel(y_column_name)\n",
        "    plt.title(f'Scatter Plot of {x_column_name} vs {y_column_name}')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Set range for x-axis if provided\n",
        "    if x_min is not None and x_max is not None:\n",
        "        plt.xlim(x_min, x_max)\n",
        "    \n",
        "    # Set range for y-axis if provided\n",
        "    if y_min is not None and y_max is not None:\n",
        "        plt.ylim(y_min, y_max)\n",
        "    \n",
        "    # Save figure with specified DPI if save_path is provided\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, dpi=dpi)\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# scatter_plot_columns(df, 'x_column', 'y_column', x_min=0, x_max=10, y_min=0, y_max=20, dpi=300, save_path='scatter_plot.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scatter_plot_columns_range(df_loc, 'lng', 'lat', x_min=-117.18, x_max=-117.14, y_min=32.7, y_max=32.775,dpi=1200,save_path='fig.png', dot_size=0.1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_values_by_group(df, group_size=4):\n",
        "    \"\"\"\n",
        "    Aggregate values in a DataFrame column containing dictionaries.\n",
        "    \n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): The DataFrame containing the dictionary column.\n",
        "        group_size (int): The number of keys to group together.\n",
        "    \n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with aggregated values.\n",
        "    \"\"\"\n",
        "    # Create a new DataFrame to store aggregated values\n",
        "    aggregated_df = pd.DataFrame()\n",
        "\n",
        "    # Iterate over each row in the original DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Get the dictionary from the row\n",
        "        dictionary = row[df.columns[0]]\n",
        "        # Create a new dictionary to store aggregated values\n",
        "        aggregated_values = {}\n",
        "        # Group the values by the keys and sum them for each group\n",
        "        for i in range(0, len(dictionary), group_size):\n",
        "            keys_subset = [str(j) for j in range(i, min(i + group_size, len(dictionary)))]\n",
        "            subset_sum = sum(dictionary.get(key, 0) for key in keys_subset)\n",
        "            aggregated_values[f\"{i//group_size + 1}th\"] = subset_sum\n",
        "        # Append the aggregated dictionary to the new DataFrame\n",
        "        aggregated_df = aggregated_df.append(aggregated_values, ignore_index=True)\n",
        "\n",
        "    return aggregated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_values_by_group(dictionary, group_size):\n",
        "    \"\"\"\n",
        "    Aggregate values in a dictionary by groups of a specified size.\n",
        "\n",
        "    Parameters:\n",
        "        dictionary (dict): The dictionary containing the values to be aggregated.\n",
        "        group_size (int): The size of each group.\n",
        "\n",
        "    Returns:\n",
        "        dict: The dictionary containing the aggregated values.\n",
        "    \"\"\"\n",
        "    \n",
        "    aggregated_dict = {}\n",
        "    for i in range(0, len(dictionary), group_size):\n",
        "        group_values = {key: dictionary[key] for key in list(dictionary.keys())[i:i+group_size]}\n",
        "        aggregated_value = sum(group_values.values())  # Sum the values for the group\n",
        "        # aggregated_dict[f\"{i//group_size + 1}th\"] = round(aggregated_value,5)\n",
        "        aggregated_dict[f\"{i}-{i+group_size} HRS\"] = round(aggregated_value,5)\n",
        "    return aggregated_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new1 = all_data_df_new.copy()\n",
        "all_data_df_new1['data_diff_aggregated'] = all_data_df_new1['data_diff'].apply(lambda x: aggregate_values_by_group(x, group_size=6))\n",
        "all_data_df_new1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new1['data_diff_aggregated'][99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new2 = all_data_df_new1.copy()\n",
        "all_data_df_new2.drop(['data_diff'], axis=1, inplace=True)\n",
        "all_data_df_new2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "all_data_df_new3 = all_data_df_new2.copy()\n",
        "all_data_df_new3 = all_data_df_new3.join(all_data_df_new3['data_diff_aggregated'].apply(lambda x: pd.Series(x)))\n",
        "all_data_df_new3.drop('data_diff_aggregated', axis=1, inplace=True)\n",
        "all_data_df_new3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('all_data_df_new.pickle', 'wb') as f:\n",
        "#     pickle.dump(all_data_df_new, f)\n",
        "\n",
        "# with open('all_data_df_new1.pickle', 'wb') as f:\n",
        "#     pickle.dump(all_data_df_new1, f)\n",
        "\n",
        "# with open('all_data_df_new2.pickle', 'wb') as f:\n",
        "#     pickle.dump(all_data_df_new2, f)\n",
        "\n",
        "# with open('all_data_df_new3.pickle', 'wb') as f:\n",
        "#     pickle.dump(all_data_df_new3, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# all_data_df_new=[]\n",
        "# all_data_df_new1=[]\n",
        "# all_data_df_new2=[]\n",
        "# all_data_df_new3=[]\n",
        "# with open('all_data_df_new.pickle', 'rb') as f:\n",
        "#     all_data_df_new = pickle.load(f)\n",
        "\n",
        "# with open('all_data_df_new1.pickle', 'rb') as f:\n",
        "#     all_data_df_new1 = pickle.load(f)\n",
        "\n",
        "# with open('all_data_df_new2.pickle', 'rb') as f:\n",
        "#     all_data_df_new2 = pickle.load(f)\n",
        "\n",
        "# with open('all_data_df_new3.pickle', 'rb') as f:\n",
        "#     all_data_df_new3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_df_new3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scatter_plot_columns_range(df_loc, 'lng', 'lat', x_min=-117.18, x_max=-117.14, y_min=32.7, y_max=32.775,dpi=1200,save_path='fig.png', dot_size=0.1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0-6 HRS\n",
            "6-12 HRS\n",
            "12-18 HRS\n",
            "18-24 HRS\n"
          ]
        }
      ],
      "source": [
        "import folium\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "def get_fill_color(my_list,value):\n",
        "    \"\"\"\n",
        "    Generate fill color based on a number between 0.1 and 0.5.\n",
        "    The color varies linearly from orange (0.1) to blue (0.5),\n",
        "    with green for the highest values.\n",
        "    \"\"\"\n",
        "    # Ensure value is within the valid range (0.1 to 0.5)\n",
        "    # min_val = min(my_list)\n",
        "    # max_val = max(my_list)\n",
        "\n",
        "    min_val = 0.1\n",
        "    max_val = 0.4\n",
        "\n",
        "    value = max(min_val, min(max_val, value))\n",
        "    \n",
        "    # Calculate the interpolation between orange and blue\n",
        "    # hue = 0.666 - 0.666 * ( (value - min_val) / (max_val - min_val) )  # Interpolate hue from blue to orange\n",
        "    hue = 0.666 * ( (value - min_val) / (max_val - min_val) )  # Interpolate hue from blue to orange\n",
        "    rgba = mcolors.hsv_to_rgb([hue, 1.0, 1.0])\n",
        "\n",
        "    # hue = np.log10(1 + (value - min_val) / (max_val - min_val)) / np.log10(2)  # Logarithmic interpolation\n",
        "    # rgba = mcolors.hsv_to_rgb([hue, 1.0, 1.0])\n",
        "    \n",
        "    # Convert RGBA color to hexadecimal format\n",
        "    hex_color = mcolors.rgb2hex(rgba)\n",
        "    \n",
        "    return hex_color\n",
        "\n",
        "num_points = len(list(all_data_df_new3['lat']))\n",
        "\n",
        "lats = list(all_data_df_new3['lat'])\n",
        "lons = list(all_data_df_new3['lng'])\n",
        "\n",
        "\n",
        "# Calculate the center of the map\n",
        "center_lat = np.mean(lats)\n",
        "center_lon = np.mean(lons)\n",
        "\n",
        "# Create a Folium map centered at the calculated center\n",
        "\n",
        "# Add the scatter plot\n",
        "def genMapParking(df):\n",
        "    column_name_list = df.columns.tolist()[3:]\n",
        "    for hours in column_name_list:\n",
        "        print(hours)\n",
        "        mymap = folium.Map(location=[center_lat, center_lon], zoom_start=15, tiles='OpenStreetMap')\n",
        "        hr_list = df[hours]\n",
        "        for lat, lon, value in zip(lats, lons, hr_list):\n",
        "            fill_color=get_fill_color(hr_list,value)\n",
        "            folium.CircleMarker(\n",
        "                location=[lat, lon],\n",
        "                radius=1,\n",
        "                # color='blue',\n",
        "                color=fill_color,\n",
        "                fill=True,\n",
        "                # fill=False,\n",
        "                # fill_color='blue',\n",
        "                fill_color=fill_color,\n",
        "                fill_opacity=1,\n",
        "                popup=f'Value: {value}'\n",
        "            ).add_to(mymap)\n",
        "\n",
        "        # Save the map to an HTML file\n",
        "        mymap.save('map_with_scatter'+hours+'.html')\n",
        "\n",
        "genMapParking(all_data_df_new3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
